{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  LoL K-Means Tier List (Notebook)\n",
        "This notebook bundles three workflows:\n",
        "1) K-Means Diagnostics, 2) Single-Patch Tierlist, 3) All-Patches Tierlist."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Dependencies ready\n"
          ]
        }
      ],
      "source": [
        "import sys, subprocess, pkgutil\n",
        "def _pip(pkg):\n",
        "    if pkg not in {m.name for m in pkgutil.iter_modules()}:\n",
        "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
        "for p in [\"pandas\",\"numpy\",\"scikit-learn\",\"matplotlib\",\"python-dotenv\"]:\n",
        "    _pip(p)\n",
        "print(\"‚úì Dependencies ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Environment setup (.env friendly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ OUT_DIR set to: /Users/brandonlee/Documents/GitHub/cs171_league_analysts_in_training/Datasets/riot_out\n",
            "üíæ .env updated at: /Users/brandonlee/Documents/GitHub/cs171_league_analysts_in_training/.env\n"
          ]
        }
      ],
      "source": [
        "# Repo-relative OUT_DIR: .../cs171_league_analysts_in_training/Datasets/riot_out\n",
        "from pathlib import Path\n",
        "import os, re\n",
        "\n",
        "repo = Path.cwd()\n",
        "while repo.name != \"cs171_league_analysts_in_training\" and repo.parent != repo:\n",
        "    repo = repo.parent\n",
        "\n",
        "DESIRED_OUT = str(repo / \"Datasets\" / \"riot_out\")\n",
        "os.environ[\"OUT_DIR\"] = DESIRED_OUT\n",
        "Path(DESIRED_OUT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "envp = repo / \".env\"\n",
        "text = envp.read_text(encoding=\"utf-8\") if envp.exists() else \"\"\n",
        "if re.search(r\"^OUT_DIR=\", text, flags=re.M):\n",
        "    text = re.sub(r\"^OUT_DIR=.*$\", f\"OUT_DIR={DESIRED_OUT}\", text, flags=re.M)\n",
        "else:\n",
        "    if text and not text.endswith(\"\\n\"): text += \"\\n\"\n",
        "    text += f\"OUT_DIR={DESIRED_OUT}\\n\"\n",
        "envp.write_text(text, encoding=\"utf-8\")\n",
        "\n",
        "print(\"‚úÖ OUT_DIR set to:\", os.getenv(\"OUT_DIR\"))\n",
        "print(\"üíæ .env updated at:\", envp.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Paths (Launcher)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OUT_DIR     : /Users/brandonlee/Documents/GitHub/cs171_league_analysts_in_training/Datasets/riot_out\n",
            "CSV_COMBINED: /Users/brandonlee/Documents/GitHub/cs171_league_analysts_in_training/Datasets/champion_winrates_all_patches.csv\n"
          ]
        }
      ],
      "source": [
        "# Paths (Launcher) ‚Äî repo-robust\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "REPO_NAME = \"cs171_league_analysts_in_training\"\n",
        "\n",
        "# Find the repo root by walking up from the current working dir\n",
        "repo = Path.cwd()\n",
        "while repo.name != REPO_NAME and repo.parent != repo:\n",
        "    repo = repo.parent\n",
        "if repo.name != REPO_NAME:\n",
        "    # Fallback: assume current dir is the repo root\n",
        "    print(f\"‚ö†Ô∏è Could not find '{REPO_NAME}' above {Path.cwd()}; using CWD as repo root.\")\n",
        "    repo = Path.cwd()\n",
        "\n",
        "# OUT_DIR inside Datasets/\n",
        "OUT_DIR = repo / \"Datasets\" / \"riot_out\"\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "os.environ[\"OUT_DIR\"] = str(OUT_DIR)\n",
        "\n",
        "# Combined CSV inside Datasets/\n",
        "CSV_COMBINED = repo / \"Datasets\" / \"champion_winrates_all_patches.csv\"\n",
        "\n",
        "print(\"OUT_DIR     :\", OUT_DIR)\n",
        "print(\"CSV_COMBINED:\", CSV_COMBINED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1)  K-Means Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMeans\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m silhouette_score\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/sklearn/__init__.py:73\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[32m     70\u001b[39m     __check_build,\n\u001b[32m     71\u001b[39m     _distributor_init,\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_show_versions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[32m     76\u001b[39m _submodules = [\n\u001b[32m     77\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcalibration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcluster\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcompose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    115\u001b[39m ]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/sklearn/base.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_metadata_requests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_missing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_scalar_nan\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_repr_html\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReprHTMLMixin, _HTMLDocumentationLinkMixin\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/sklearn/utils/__init__.py:9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m metadata_routing\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_chunking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_indexing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     _safe_indexing,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     18\u001b[39m     resample,\n\u001b[32m     19\u001b[39m     shuffle,\n\u001b[32m     20\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/sklearn/utils/_chunking.py:11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_param_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvalidation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/sklearn/utils/_array_api.py:20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array_api_extra \u001b[38;5;28;01mas\u001b[39;00m xpx\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexternals\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01marray_api_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m numpy \u001b[38;5;28;01mas\u001b[39;00m np_compat\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfixes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TODO: complete __all__\u001b[39;00m\n\u001b[32m     23\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mxpx\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# we import xpx here just to re-export it, need this to appease ruff\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/sklearn/utils/fixes.py:16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msparse\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optimize\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/scipy/stats/__init__.py:626\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m \n\u001b[32m    622\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    625\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/scipy/stats/_stats_py.py:40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m milp, LinearConstraint\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, _get_nan,\n\u001b[32m     44\u001b[39m                               _rename_parameter, _contains_nan,\n\u001b[32m     45\u001b[39m                               normalize_axis_index, np_vecdot, AxisError)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/cs171_league_analysts_in_training/.venv/lib/python3.12/site-packages/scipy/spatial/__init__.py:112\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_kdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_qhull\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_spherical_voronoi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SphericalVoronoi\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_plotutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
            "\u001b[36mFile \u001b[39m\u001b[32mscipy/spatial/_qhull.pyx:1\u001b[39m, in \u001b[36minit scipy.spatial._qhull\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:645\u001b[39m, in \u001b[36mparent\u001b[39m\u001b[34m(self)\u001b[39m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "kmeans_cluster_diagnostics.py\n",
        "Outputs KMeans \"loss\" graphs for LoL champion data:\n",
        "- Elbow curve (Sum of Square Errors) vs Number of clusters\n",
        "- (Optional) Silhouette score vs K\n",
        "\n",
        "Usage:\n",
        "  python3 kmeans_cluster_diagnostics.py \\\n",
        "    --csv \"/path/to/champion_winrates_all_patches.csv\" \\\n",
        "    --patch 15.20 \\\n",
        "    --k-min 2 --k-max 10 --logit \\\n",
        "    --out-dir \"/path/to/riot_out/plots\"\n",
        "\n",
        "  # All patches\n",
        "  python3 kmeans_cluster_diagnostics.py --csv ... --each --k-min 2 --k-max 10\n",
        "\n",
        "Notes:\n",
        "- Features: win_rate, pick_rate, ban_rate (percent). Optional --logit transform.\n",
        "- Always scales features with StandardScaler.\n",
        "- Weights KMeans by sqrt(games) unless --no-weight is passed.\n",
        "- Saves: elbow_sse_patch_<patch>.png  (and silhouette_patch_<patch>.png unless --no-sil is set)\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def logit_percent(p, eps=1e-4):\n",
        "    p = np.clip(np.asarray(p, dtype=float) / 100.0, eps, 1 - eps)\n",
        "    return np.log(p / (1 - p))\n",
        "\n",
        "\n",
        "def canon_patch(p):\n",
        "    s = str(p).strip()\n",
        "    m = re.search(r'(\\d+)\\.(\\d+)', s)\n",
        "    return f\"{int(m.group(1))}.{int(m.group(2))}\" if m else None\n",
        "\n",
        "\n",
        "def numeric_patch_key(p):\n",
        "    return tuple(map(int, p.split(\".\")))\n",
        "\n",
        "\n",
        "def prepare_features(df, use_logit):\n",
        "    feats = df[[\"win_rate\", \"pick_rate\", \"ban_rate\"]].to_numpy(dtype=float)\n",
        "    if use_logit:\n",
        "        feats = np.column_stack([\n",
        "            logit_percent(df[\"win_rate\"].values),\n",
        "            logit_percent(df[\"pick_rate\"].values),\n",
        "            logit_percent(df[\"ban_rate\"].values),\n",
        "        ])\n",
        "    X = StandardScaler().fit_transform(feats)\n",
        "    return X\n",
        "\n",
        "\n",
        "def run_for_patch(df, patch, k_min, k_max, use_logit, weight_by_games, out_dir: Path, save_sil=True, random_state=42):\n",
        "    dpp = df[df[\"patch\"] == patch].copy()\n",
        "    if dpp.empty:\n",
        "        print(f\"[skip] patch {patch}: no rows after cleaning\")\n",
        "        return None\n",
        "\n",
        "    # Prepare features\n",
        "    X = prepare_features(dpp, use_logit=use_logit)\n",
        "    n = len(dpp)\n",
        "    if n < 3:\n",
        "        print(f\"[skip] patch {patch}: too few rows ({n})\")\n",
        "        return None\n",
        "\n",
        "    # sample weights\n",
        "    sample_weight = None\n",
        "    if weight_by_games and \"games\" in dpp.columns:\n",
        "        sample_weight = np.sqrt(np.clip(pd.to_numeric(dpp[\"games\"], errors=\"coerce\").fillna(1).values, 1, None))\n",
        "\n",
        "    # Respect k bounds\n",
        "    k_max_eff = max(k_min, min(k_max, n - 1))\n",
        "\n",
        "    ks, inertias, sils = [], [], []\n",
        "    for k in range(k_min, k_max_eff + 1):\n",
        "        try:\n",
        "            km = KMeans(n_clusters=k, n_init=20, random_state=random_state)\n",
        "            km.fit(X, sample_weight=sample_weight)\n",
        "            ks.append(k)\n",
        "            inertias.append(km.inertia_)\n",
        "            if save_sil and k >= 2 and k < n:\n",
        "                sils.append(silhouette_score(X, km.labels_))\n",
        "            elif save_sil:\n",
        "                sils.append(np.nan)\n",
        "            print(f\"[{patch}] K={k:2d}  SSE={km.inertia_:,.1f}\" + (f\"  sil={sils[-1]: .4f}\" if save_sil else \"\"))\n",
        "        except Exception as e:\n",
        "            print(f\"[{patch}] K={k}: skipping ({e})\")\n",
        "\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Elbow plot: match the example labels\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(ks, inertias, marker=\"o\")\n",
        "    plt.xlabel(\"Number of clusters\")\n",
        "    plt.ylabel(\"Sum of Square Errors\")\n",
        "    plt.title(f\"Elbow Curve ‚Äî Patch {patch}\")\n",
        "    elbow_png = out_dir / f\"elbow_sse_patch_{patch}.png\"\n",
        "    plt.savefig(elbow_png, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"[saved] {elbow_png}\")\n",
        "\n",
        "    # Optional silhouette\n",
        "    if save_sil:\n",
        "        valid = [(k, s) for k, s in zip(ks, sils) if not np.isnan(s)]\n",
        "        if valid:\n",
        "            kv, sv = zip(*valid)\n",
        "            plt.figure(figsize=(10, 4))\n",
        "            plt.plot(kv, sv, marker=\"o\")\n",
        "            plt.xlabel(\"Number of clusters\")\n",
        "            plt.ylabel(\"Silhouette Score\")\n",
        "            plt.title(f\"Silhouette vs K ‚Äî Patch {patch}\")\n",
        "            sil_png = out_dir / f\"silhouette_patch_{patch}.png\"\n",
        "            plt.savefig(sil_png, bbox_inches=\"tight\")\n",
        "            plt.close()\n",
        "            print(f\"[saved] {sil_png}\")\n",
        "\n",
        "    # CSV summary\n",
        "    import csv\n",
        "    diag_csv = out_dir / f\"kmeans_diagnostics_patch_{patch}.csv\"\n",
        "    with diag_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.writer(f)\n",
        "        w.writerow([\"patch\", \"K\", \"SSE\", \"silhouette\"])\n",
        "        for i, k in enumerate(ks):\n",
        "            sse = inertias[i]\n",
        "            sil = sils[i] if save_sil and i < len(sils) else \"\"\n",
        "            w.writerow([patch, k, sse, sil])\n",
        "    print(f\"[saved] {diag_csv}\")\n",
        "\n",
        "    return {\"patch\": patch, \"K\": ks, \"SSE\": inertias, \"silhouette\": sils if save_sil else None}\n",
        "\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"KMeans elbow (SSE) and silhouette diagnostics\")\n",
        "    ap.add_argument(\"--csv\", required=True, help=\"Combined CSV (patch,win_rate,pick_rate,ban_rate,games)\")\n",
        "    ap.add_argument(\"--patch\", help=\"Patch to analyze (e.g., 15.20). If omitted and --each not set, use latest numerically.\")\n",
        "    ap.add_argument(\"--each\", action=\"store_true\", help=\"Run for every patch separately\")\n",
        "    ap.add_argument(\"--k-min\", type=int, default=2, help=\"Minimum K\")\n",
        "    ap.add_argument(\"--k-max\", type=int, default=10, help=\"Maximum K\")\n",
        "    ap.add_argument(\"--logit\", action=\"store_true\", help=\"Apply logit transform to WR/PR/BR before scaling\")\n",
        "    ap.add_argument(\"--no-weight\", action=\"store_true\", help=\"Disable sqrt(games) sample weighting\")\n",
        "    ap.add_argument(\"--no-sil\", action=\"store_true\", help=\"Do not compute/save silhouette plot\")\n",
        "    ap.add_argument(\"--out-dir\", default=os.path.expanduser(\"~/riot_out/plots\"), help=\"Output directory\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    out_dir = Path(args.out_dir)\n",
        "\n",
        "    # Read CSV as string to preserve \"15.20\"\n",
        "    df = pd.read_csv(args.csv, dtype={\"patch\": str})\n",
        "    print(f\"[debug] loaded rows: {len(df)} from {args.csv}\")\n",
        "\n",
        "    # Normalize patch\n",
        "    df[\"patch\"] = df[\"patch\"].map(canon_patch)\n",
        "\n",
        "    # Ensure columns exist + numeric\n",
        "    if \"ban_rate\" not in df.columns: df[\"ban_rate\"] = 0.0\n",
        "    if \"games\" not in df.columns: df[\"games\"] = 1\n",
        "    for c in [\"win_rate\", \"pick_rate\", \"ban_rate\", \"games\"]:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    df = df.dropna(subset=[\"patch\",\"win_rate\",\"pick_rate\",\"ban_rate\",\"games\"]).copy()\n",
        "\n",
        "    patches = sorted(df[\"patch\"].unique(), key=numeric_patch_key)\n",
        "    print(\"[debug] patches after cleaning:\", patches)\n",
        "\n",
        "    if args.each:\n",
        "        for p in patches:\n",
        "            sub = out_dir / f\"patch_{p}\"\n",
        "            run_for_patch(df, p, args.k_min, args.k_max, args.logit, (not args.no_weight), sub, save_sil=(not args.no_sil))\n",
        "    else:\n",
        "        target = canon_patch(args.patch) if args.patch else patches[-1] if patches else None\n",
        "        if not target:\n",
        "            raise SystemExit(\"No patches after cleaning.\")\n",
        "        print(\"[debug] selecting patch:\", target)\n",
        "        run_for_patch(df, target, args.k_min, args.k_max, args.logit, (not args.no_weight), out_dir, save_sil=(not args.no_sil))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ñ∂Ô∏è Diagnostics Launcher\n",
        "import sys, runpy, os\n",
        "CSV=CSV_COMBINED; OUT=OUT_DIR+\"/diagnostics\"; KMIN, KMAX = 2, 10; LOGIT=True; PATCH=None\n",
        "argv = [\"kmeans_cluster_diagnostics.py\",\"--csv\",CSV,\"--out-dir\",OUT,\"--kmin\",str(KMIN),\"--kmax\",str(KMAX)]\n",
        "if LOGIT: argv.append(\"--logit\")\n",
        "if PATCH: argv += [\"--patch\", PATCH]\n",
        "sys.argv = argv\n",
        "runpy.run_path(\"/mnt/data/kmeans_cluster_diagnostics.py\", run_name=\"__main__\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2)  Single-Patch K-Means Tierlist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "import argparse, os, subprocess\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import re\n",
        "\n",
        "\n",
        "TIERS_5 = [\"S\",\"A\",\"B\",\"C\",\"D\"]\n",
        "TIERS_6 = [\"S\",\"A\",\"B\",\"C\",\"D\",\"E\"]\n",
        "TIERS_7 = [\"S\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"]\n",
        "\n",
        "def logit_percent(p, eps=1e-4):\n",
        "    # p is in % (0..100). Convert to logit in R.\n",
        "    p = np.clip(p / 100.0, eps, 1 - eps)\n",
        "    return np.log(p / (1 - p))\n",
        "\n",
        "def choose_tier_labels(k):\n",
        "    if k == 5: return TIERS_5\n",
        "    if k == 6: return TIERS_6\n",
        "    if k == 7: return TIERS_7\n",
        "    # fallback: T1..Tk (T1 = best)\n",
        "    return [f\"T{i}\" for i in range(1, k+1)]\n",
        "\n",
        "def rank_clusters_by_center(centers_raw_pct, weights=(0.6, 0.3, 0.1)):\n",
        "    \"\"\"\n",
        "    Rank clusters by a composite score computed in **raw % space** (WR, PR, BR).\n",
        "    centers_raw_pct: array shape (k, 3) for [wr%, pr%, br%]\n",
        "    \"\"\"\n",
        "    w_wr, w_pr, w_br = weights\n",
        "    scores = w_wr*centers_raw_pct[:,0] + w_pr*centers_raw_pct[:,1] + w_br*centers_raw_pct[:,2]\n",
        "    order = np.argsort(-scores)  # descending\n",
        "    rank_of_cluster = np.empty_like(order)\n",
        "    rank_of_cluster[order] = np.arange(len(order))  # 0 = best\n",
        "    return rank_of_cluster, scores\n",
        "\n",
        "def cluster_one_patch(df_patch, k, use_logit=False, weight_by_games=True, random_state=42):\n",
        "    \"\"\"\n",
        "    df_patch columns: championId, championName, win_rate, pick_rate, ban_rate, games\n",
        "    Returns: per-row labels, tier letters, centers (raw %) and mapping.\n",
        "    \"\"\"\n",
        "    feats = df_patch[[\"win_rate\",\"pick_rate\",\"ban_rate\"]].to_numpy(dtype=float)\n",
        "    # Keep a copy in % space for center back-transform\n",
        "    feats_pct = feats.copy()\n",
        "\n",
        "    # Optional transform then scale\n",
        "    if use_logit:\n",
        "        feats = np.column_stack([logit_percent(df_patch[\"win_rate\"].values),\n",
        "                                 logit_percent(df_patch[\"pick_rate\"].values),\n",
        "                                 logit_percent(df_patch[\"ban_rate\"].values)])\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(feats)\n",
        "\n",
        "    # Sample weights (downweight low-sample champs)\n",
        "    sample_weight = None\n",
        "    if weight_by_games and \"games\" in df_patch.columns:\n",
        "        # sqrt or log1p temper extremes; choose one:\n",
        "        sample_weight = np.sqrt(np.clip(df_patch[\"games\"].values, 1, None))\n",
        "        # sample_weight = np.log1p(df_patch[\"games\"].values)\n",
        "\n",
        "    # KMeans\n",
        "    km = KMeans(n_clusters=k, n_init=20, random_state=random_state)\n",
        "    km.fit(X, sample_weight=sample_weight)\n",
        "    labels = km.labels_\n",
        "\n",
        "    # Compute cluster centers back in **raw %** units (for ranking)\n",
        "    centers_in_feat_space = scaler.inverse_transform(km.cluster_centers_)\n",
        "    if use_logit:\n",
        "        # inverse-logit to %: sigmoid(x)*100\n",
        "        sigmoid = lambda z: 1.0/(1.0+np.exp(-z))\n",
        "        centers_raw_pct = sigmoid(centers_in_feat_space) * 100.0\n",
        "    else:\n",
        "        centers_raw_pct = centers_in_feat_space  # already roughly in % units\n",
        "\n",
        "    # Rank clusters -> tiers\n",
        "    rank_of_cluster, scores = rank_clusters_by_center(centers_raw_pct)\n",
        "    tiers = choose_tier_labels(k)\n",
        "    cluster_to_tier = {c: tiers[rank_of_cluster[c]] for c in range(k)}\n",
        "\n",
        "    return labels, cluster_to_tier, centers_raw_pct, scores\n",
        "\n",
        "def run_for_patch(df, patch, k, use_logit, weight_by_games, out_dir):\n",
        "    dfp = df[df[\"patch\"] == patch].copy()\n",
        "    if dfp.empty:\n",
        "        print(f\"[skip] patch {patch}: no rows\")\n",
        "        return None, None\n",
        "\n",
        "    labels, c2t, centers_raw_pct, scores = cluster_one_patch(\n",
        "        dfp, k=k, use_logit=use_logit, weight_by_games=weight_by_games\n",
        "    )\n",
        "    dfp[\"cluster\"] = labels\n",
        "    dfp[\"tier\"] = dfp[\"cluster\"].map(c2t)\n",
        "\n",
        "    # Save tier list\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    out_csv = out_dir / f\"tierlist_patch_{patch}.csv\"\n",
        "    cols = [\"patch\",\"championId\",\"championName\",\"win_rate\",\"pick_rate\",\"ban_rate\",\"games\",\"cluster\",\"tier\"]\n",
        "    dfp[cols].to_csv(out_csv, index=False)\n",
        "    print(f\"[saved] {out_csv}\")\n",
        "\n",
        "    # Save cluster centers\n",
        "    centers_df = pd.DataFrame(centers_raw_pct, columns=[\"center_wr_pct\",\"center_pr_pct\",\"center_br_pct\"])\n",
        "    centers_df[\"cluster\"] = np.arange(len(centers_df))\n",
        "    centers_df[\"score\"] = scores\n",
        "    centers_df[\"tier\"] = centers_df[\"cluster\"].map(c2t)\n",
        "    centers_df[\"patch\"] = patch\n",
        "    centers_csv = out_dir / f\"tier_centers_patch_{patch}.csv\"\n",
        "    centers_df[[\"patch\",\"cluster\",\"tier\",\"center_wr_pct\",\"center_pr_pct\",\"center_br_pct\",\"score\"]].to_csv(centers_csv, index=False)\n",
        "    print(f\"[saved] {centers_csv}\")\n",
        "\n",
        "    return dfp, centers_df\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"K-means tier list from WR/PR/BR (per patch)\")\n",
        "    ap.add_argument(\"--csv\", required=True, help=\"Input CSV with columns: patch, championId, championName, win_rate, pick_rate, ban_rate, games\")\n",
        "    ap.add_argument(\"--k\", type=int, default=5, help=\"Number of tiers/clusters (default 5)\")\n",
        "    ap.add_argument(\"--patch\", default=None, help=\"Specific patch (e.g., '15.22'). If omitted and --each not set, uses latest.\")\n",
        "    ap.add_argument(\"--each\", action=\"store_true\", help=\"Cluster each patch separately and save multiple tierlists\")\n",
        "    ap.add_argument(\"--logit\", action=\"store_true\", help=\"Use logit transform on rates before scaling (often better)\")\n",
        "    ap.add_argument(\"--no-weight\", action=\"store_true\", help=\"Disable games-based sample weighting\")\n",
        "    ap.add_argument(\"--out-dir\", default=os.path.expanduser(\"~/riot_out/tierlists\"))\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
        "    def git_root(start: Path) -> Path | None:\n",
        "        try:\n",
        "            p = subprocess.check_output(\n",
        "                [\"git\", \"rev-parse\", \"--show-toplevel\"],\n",
        "                cwd=start\n",
        "            ).decode().strip()\n",
        "            return Path(p)\n",
        "        except Exception:\n",
        "            return None\n",
        "    \n",
        "    REPO_ROOT = git_root(SCRIPT_DIR)\n",
        "\n",
        "    out_dir = Path(os.getenv(\"out_dir\", REPO_ROOT / \"riot_out\"))\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"[out] saving to: {out_dir}\")\n",
        "    df = pd.read_csv(args.csv, dtype={\"patch\": str})  # preserve \"15.20\"\n",
        "\n",
        "    print(f\"[debug] loaded rows: {len(df)} from {args.csv}\")\n",
        "\n",
        "    # 1) normalize patch ‚Üí \"major.minor\"\n",
        "    def canon_patch(p):\n",
        "        s = str(p).strip()\n",
        "        m = re.search(r'(\\d+)\\.(\\d+)', s)\n",
        "        return f\"{int(m.group(1))}.{int(m.group(2))}\" if m else None\n",
        "    df[\"patch\"] = df[\"patch\"].map(canon_patch)\n",
        "\n",
        "    # 2) ensure required columns exist\n",
        "    if \"ban_rate\" not in df.columns:\n",
        "        df[\"ban_rate\"] = 0.0\n",
        "    if \"games\" not in df.columns:\n",
        "        df[\"games\"] = 1\n",
        "\n",
        "    # 3) numeric coercion\n",
        "    for c in [\"win_rate\",\"pick_rate\",\"ban_rate\",\"games\"]:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    # 4) drop rows that can't be used\n",
        "    df = df.dropna(subset=[\"patch\",\"win_rate\",\"pick_rate\",\"ban_rate\",\"games\"]).copy()\n",
        "\n",
        "    # 5) show which patches remain\n",
        "    # 3) when picking the latest patch, sort NUMERICALLY\n",
        "    patches = sorted(df[\"patch\"].unique(), key=lambda p: tuple(map(int, p.split(\".\"))))\n",
        "    # if --patch not provided, choose the max numerically:\n",
        "    target_patch = args.patch or patches[-1]\n",
        "    print(\"[debug] patches after cleaning:\", patches)\n",
        "    print(\"[debug] counts by patch:\\n\", df[\"patch\"].value_counts().sort_index())\n",
        "\n",
        "    if args.each:\n",
        "        all_rows, all_centers = [], []\n",
        "        for p in patches:\n",
        "            res = run_for_patch(df, p, args.k, args.logit, not args.no_weight, out_dir)\n",
        "            if res[0] is not None:\n",
        "                all_rows.append(res[0]); all_centers.append(res[1])\n",
        "        if all_rows:\n",
        "            pd.concat(all_rows).to_csv(out_dir / \"tierlist_all_patches.csv\", index=False)\n",
        "            pd.concat(all_centers).to_csv(out_dir / \"tier_centers_all_patches.csv\", index=False)\n",
        "            print(f\"[saved] {out_dir/'tierlist_all_patches.csv'}\")\n",
        "            print(f\"[saved] {out_dir/'tier_centers_all_patches.csv'}\")\n",
        "        pass\n",
        "    else:\n",
        "        target_patch = canon_patch(args.patch) if args.patch else patches[-1]\n",
        "        run_for_patch(df, target_patch, args.k, args.logit, not args.no_weight, out_dir)\n",
        "        print(\"[debug] selecting patch:\", target_patch)\n",
        "        run_for_patch(df, target_patch, args.k, args.logit, not args.no_weight, out_dir)\n",
        "        return\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ñ∂Ô∏è Single-Patch Launcher\n",
        "import sys, runpy\n",
        "CSV=CSV_COMBINED; PATCH=\"15.20\"; K=5; LOGIT=True\n",
        "argv=[\"kmeans_tierlist.py\",\"--csv\",CSV,\"--k\",str(K),\"--out-dir\",OUT_DIR]\n",
        "if PATCH: argv+=[\"--patch\",PATCH]\n",
        "if LOGIT: argv.append(\"--logit\")\n",
        "sys.argv=argv\n",
        "runpy.run_path(\"/mnt/data/kmeans_tierlist.py\", run_name=\"__main__\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3)  All-Patches K-Means Tierlist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "tierlist_all_patches.py\n",
        "Build KMeans-based tier lists for EVERY patch in a combined CSV.\n",
        "\n",
        "Inputs (one row per champion per patch):\n",
        "  patch, championId, championName, games, wins, win_rate, pick_rate, ban_rate\n",
        "\n",
        "Outputs:\n",
        "  <OUT_DIR>/patch_<patch>/tierlist_patch_<patch>.csv\n",
        "  <OUT_DIR>/patch_<patch>/tier_centers_patch_<patch>.csv\n",
        "  <OUT_DIR>/patch_<patch>/kmeans_validation_<patch>.csv\n",
        "  <OUT_DIR>/tierlist_all_patches.csv\n",
        "  <OUT_DIR>/kmeans_validation_all_patches.csv\n",
        "\n",
        "CLI:\n",
        "  python3 tierlist_all_patches.py \\\n",
        "    --csv /path/to/champion_winrates_all_patches.csv \\\n",
        "    --k 5 --logit --out-dir /path/to/riot_out\n",
        "\"\"\"\n",
        "\n",
        "import argparse, os, re, csv\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    silhouette_score, davies_bouldin_score, calinski_harabasz_score,\n",
        "    adjusted_rand_score,\n",
        ")\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "# ----------------------------- utils -----------------------------\n",
        "\n",
        "def canon_patch(s: str) -> str | None:\n",
        "    \"\"\"Normalize '15.20' from any '15.20.x' / loose strings.\"\"\"\n",
        "    s = str(s).strip()\n",
        "    m = re.search(r'(\\d+)\\.(\\d+)', s)\n",
        "    return f\"{int(m.group(1))}.{int(m.group(2))}\" if m else None\n",
        "\n",
        "def numeric_patch_key(p: str) -> tuple[int, int]:\n",
        "    return tuple(map(int, p.split(\".\")))\n",
        "\n",
        "def logit_percent(arr, eps=1e-4):\n",
        "    \"\"\"Logit transform percent features (stabilize extremes).\"\"\"\n",
        "    p = np.clip(np.asarray(arr, dtype=float) / 100.0, eps, 1 - eps)\n",
        "    return np.log(p / (1 - p))\n",
        "\n",
        "def prepare_features(df: pd.DataFrame, use_logit: bool) -> np.ndarray:\n",
        "    cols = [\"win_rate\", \"pick_rate\", \"ban_rate\"]\n",
        "    X_raw = (\n",
        "        np.column_stack([logit_percent(df[c].values) for c in cols])\n",
        "        if use_logit else\n",
        "        df[cols].to_numpy(dtype=float)\n",
        "    )\n",
        "    return StandardScaler().fit_transform(X_raw)\n",
        "\n",
        "def tier_letters(k: int) -> list[str]:\n",
        "    base = [\"S\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\"]\n",
        "    if k <= len(base): return base[:k]\n",
        "    return base + [f\"T{i}\" for i in range(k - len(base))]\n",
        "\n",
        "\n",
        "# ------------------------- diagnostics --------------------------\n",
        "\n",
        "def kmeans_diagnostics(X: np.ndarray, labels: np.ndarray, df_with_tier: pd.DataFrame) -> dict:\n",
        "    \"\"\"Internal quality + construct validity for one patch.\"\"\"\n",
        "    uniq = np.unique(labels)\n",
        "    if len(uniq) > 1:\n",
        "        sil = silhouette_score(X, labels)\n",
        "        db  = davies_bouldin_score(X, labels)\n",
        "        ch  = calinski_harabasz_score(X, labels)\n",
        "    else:\n",
        "        sil = db = ch = np.nan\n",
        "\n",
        "    # Monotonicity of mean WR across tiers (S > A > B ...). Higher tier listed first.\n",
        "    g = (df_with_tier.groupby(\"tier\")[\"win_rate\"]\n",
        "         .mean()\n",
        "         .sort_values(ascending=False))\n",
        "    mono = bool(all(g.values[i] >= g.values[i+1] for i in range(len(g)-1))) if len(g) > 1 else True\n",
        "\n",
        "    # How much WR variance tiers explain (quick R^2)\n",
        "    D = pd.get_dummies(df_with_tier[\"tier\"], drop_first=True)\n",
        "    y = pd.to_numeric(df_with_tier[\"win_rate\"], errors=\"coerce\").fillna(0).values\n",
        "    Xr = D.values if D.shape[1] else np.zeros((len(df_with_tier), 1))\n",
        "    r2 = LinearRegression().fit(Xr, y).score(Xr, y) if len(df_with_tier) > 1 else np.nan\n",
        "\n",
        "    return {\n",
        "        \"silhouette\": float(sil),\n",
        "        \"davies_bouldin\": float(db),\n",
        "        \"calinski_harabasz\": float(ch),\n",
        "        \"tier_monotone\": mono,\n",
        "        \"tier_wr_r2\": float(r2),\n",
        "    }\n",
        "\n",
        "def stability_ari(X: np.ndarray, k: int, seeds=(1,11,21,31,41), sample_weight=None) -> float:\n",
        "    \"\"\"Mean Adjusted Rand Index across multiple random seeds.\"\"\"\n",
        "    label_sets = []\n",
        "    for s in seeds:\n",
        "        km = KMeans(n_clusters=k, n_init=20, random_state=s)\n",
        "        km.fit(X, sample_weight=sample_weight)\n",
        "        label_sets.append(km.labels_)\n",
        "    aris = []\n",
        "    for i in range(len(label_sets)):\n",
        "        for j in range(i+1, len(label_sets)):\n",
        "            aris.append(adjusted_rand_score(label_sets[i], label_sets[j]))\n",
        "    return float(np.mean(aris)) if aris else np.nan\n",
        "\n",
        "\n",
        "# --------------------------- core -------------------------------\n",
        "\n",
        "def run_for_patch(df_patch: pd.DataFrame, patch: str, k: int, use_logit: bool,\n",
        "                  weight_by_games: bool, out_dir: Path, random_state=42):\n",
        "    \"\"\"Cluster one patch, save CSVs, return (rows, centers, diag).\"\"\"\n",
        "    if len(df_patch) < k:\n",
        "        print(f\"[skip] patch {patch}: rows={len(df_patch)} < k={k}\")\n",
        "        return None, None, None\n",
        "\n",
        "    # Prepare features\n",
        "    X = prepare_features(df_patch, use_logit=use_logit)\n",
        "\n",
        "    # Sample weighting by sqrt(games)\n",
        "    sample_weight = None\n",
        "    if weight_by_games and \"games\" in df_patch.columns:\n",
        "        sample_weight = np.sqrt(\n",
        "            np.clip(pd.to_numeric(df_patch[\"games\"], errors=\"coerce\").fillna(1).values, 1, None)\n",
        "        )\n",
        "\n",
        "    # Fit KMeans\n",
        "    km = KMeans(n_clusters=k, n_init=20, random_state=random_state)\n",
        "    km.fit(X, sample_weight=sample_weight)\n",
        "    labels = km.labels_\n",
        "\n",
        "    # Compute cluster centers in ORIGINAL % space (weighted by games if enabled)\n",
        "    centers = []\n",
        "    for c in range(k):\n",
        "        members = (labels == c)\n",
        "        n = int(members.sum())\n",
        "        if n == 0:\n",
        "            wmr = wpr = wbr = 0.0\n",
        "        else:\n",
        "            if weight_by_games and \"games\" in df_patch.columns:\n",
        "                ws = df_patch.loc[members, \"games\"].clip(lower=1).astype(float).values\n",
        "                wmr = float(np.average(df_patch.loc[members, \"win_rate\"].values, weights=ws))\n",
        "                wpr = float(np.average(df_patch.loc[members, \"pick_rate\"].values, weights=ws))\n",
        "                wbr = float(np.average(df_patch.loc[members, \"ban_rate\"].values,  weights=ws))\n",
        "            else:\n",
        "                wmr = float(df_patch.loc[members, \"win_rate\"].mean())\n",
        "                wpr = float(df_patch.loc[members, \"pick_rate\"].mean())\n",
        "                wbr = float(df_patch.loc[members, \"ban_rate\"].mean())\n",
        "        centers.append({\n",
        "            \"cluster\": c, \"n\": n,\n",
        "            \"mean_win_rate\": wmr, \"mean_pick_rate\": wpr, \"mean_ban_rate\": wbr\n",
        "        })\n",
        "\n",
        "    # Rank clusters by mean win_rate (then pick_rate) ‚Üí assign S/A/B...\n",
        "    centers_sorted = sorted(centers, key=lambda d: (-d[\"mean_win_rate\"], -d[\"mean_pick_rate\"]))\n",
        "    letters = tier_letters(k)\n",
        "    cluster_to_tier = {cinfo[\"cluster\"]: (letters[i] if i < len(letters) else f\"T{i}\")\n",
        "                       for i, cinfo in enumerate(centers_sorted)}\n",
        "\n",
        "    # Per-row output\n",
        "    rows_out = []\n",
        "    dfp = df_patch.reset_index(drop=True)\n",
        "    for i, row in dfp.iterrows():\n",
        "        c = int(labels[i])\n",
        "        rows_out.append({\n",
        "            \"patch\": patch,\n",
        "            \"championId\": int(row[\"championId\"]) if str(row[\"championId\"]).isdigit() else row[\"championId\"],\n",
        "            \"championName\": row.get(\"championName\", \"\"),\n",
        "            \"games\": int(pd.to_numeric(row.get(\"games\", 0), errors=\"coerce\")) if pd.notna(row.get(\"games\", None)) else 0,\n",
        "            \"wins\": int(pd.to_numeric(row.get(\"wins\", 0), errors=\"coerce\")) if pd.notna(row.get(\"wins\", None)) else 0,\n",
        "            \"win_rate\": float(pd.to_numeric(row.get(\"win_rate\", 0.0), errors=\"coerce\")),\n",
        "            \"pick_rate\": float(pd.to_numeric(row.get(\"pick_rate\", 0.0), errors=\"coerce\")),\n",
        "            \"ban_rate\": float(pd.to_numeric(row.get(\"ban_rate\", 0.0), errors=\"coerce\")),\n",
        "            \"cluster\": c,\n",
        "            \"tier\": cluster_to_tier.get(c, \"U\"),\n",
        "        })\n",
        "\n",
        "    # Diagnostics\n",
        "    df_out = pd.DataFrame(rows_out)\n",
        "    diag = kmeans_diagnostics(X, labels, df_out)\n",
        "    diag[\"patch\"] = patch\n",
        "    diag[\"k\"] = k\n",
        "    diag[\"n_rows\"] = int(len(df_out))\n",
        "    diag[\"stability_ari\"] = stability_ari(X, k, seeds=(1,11,21,31,41), sample_weight=sample_weight)\n",
        "\n",
        "    # Save per-patch outputs\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    per_patch = out_dir / f\"tierlist_patch_{patch}.csv\"\n",
        "    with per_patch.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=[\n",
        "            \"patch\",\"championId\",\"championName\",\"games\",\"wins\",\n",
        "            \"win_rate\",\"pick_rate\",\"ban_rate\",\"cluster\",\"tier\"\n",
        "        ])\n",
        "        w.writeheader()\n",
        "        w.writerows(rows_out)\n",
        "\n",
        "    centers_csv = out_dir / f\"tier_centers_patch_{patch}.csv\"\n",
        "    with centers_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=[\"cluster\",\"n\",\"mean_win_rate\",\"mean_pick_rate\",\"mean_ban_rate\"])\n",
        "        w.writeheader()\n",
        "        w.writerows(centers_sorted)\n",
        "\n",
        "    diag_csv = out_dir / f\"kmeans_validation_{patch}.csv\"\n",
        "    pd.DataFrame([diag]).to_csv(diag_csv, index=False)\n",
        "\n",
        "    print(f\"[saved] {per_patch}\")\n",
        "    print(f\"[saved] {centers_csv}\")\n",
        "    print(f\"[saved] {diag_csv}\")\n",
        "\n",
        "    return rows_out, centers_sorted, diag\n",
        "\n",
        "\n",
        "# ---------------------------- main ------------------------------\n",
        "\n",
        "def main():\n",
        "    ap = argparse.ArgumentParser(description=\"KMeans tierlist for every patch in the combined CSV\")\n",
        "    ap.add_argument(\"--csv\", required=True, help=\"Combined CSV path (from scraper)\")\n",
        "    ap.add_argument(\"--k\", type=int, default=5, help=\"Clusters per patch (default 5)\")\n",
        "    ap.add_argument(\"--logit\", action=\"store_true\", help=\"Apply logit transform to WR/PR/BR before scaling\")\n",
        "    ap.add_argument(\"--no-weight\", action=\"store_true\", help=\"Disable sqrt(games) sample weighting\")\n",
        "    ap.add_argument(\"--out-dir\", default=os.path.expanduser(\"~/riot_out\"), help=\"Output directory\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    out_dir = Path(args.out_dir)\n",
        "\n",
        "    # Load & clean data\n",
        "    df = pd.read_csv(args.csv, dtype={\"patch\": str})\n",
        "    print(f\"[debug] loaded rows: {len(df)} from {args.csv}\")\n",
        "    df[\"patch\"] = df[\"patch\"].map(canon_patch)\n",
        "\n",
        "    if \"ban_rate\" not in df.columns: df[\"ban_rate\"] = 0.0\n",
        "    if \"games\" not in df.columns: df[\"games\"] = 1\n",
        "\n",
        "    for c in [\"win_rate\",\"pick_rate\",\"ban_rate\",\"games\",\"wins\",\"championId\"]:\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    df = df.dropna(subset=[\"patch\",\"win_rate\",\"pick_rate\",\"ban_rate\"]).copy()\n",
        "    patches = sorted(df[\"patch\"].unique(), key=numeric_patch_key)\n",
        "    print(\"[debug] patches after cleaning:\", patches)\n",
        "\n",
        "    all_rows, all_diags = [], []\n",
        "\n",
        "    for p in patches:\n",
        "        dfp = df[df[\"patch\"] == p].copy()\n",
        "        sub_out = out_dir / f\"patch_{p}\"\n",
        "        rows, _, diag = run_for_patch(\n",
        "            df_patch = dfp,\n",
        "            patch = p,\n",
        "            k = args.k,\n",
        "            use_logit = args.logit,\n",
        "            weight_by_games = (not args.no_weight),\n",
        "            out_dir = sub_out\n",
        "        )\n",
        "        if rows:\n",
        "            all_rows.extend(rows)\n",
        "        if diag:\n",
        "            all_diags.append(diag)\n",
        "\n",
        "    # Save combined outputs\n",
        "    if all_rows:\n",
        "        combined_csv = out_dir / \"tierlist_all_patches.csv\"\n",
        "        with combined_csv.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            w = csv.DictWriter(f, fieldnames=[\n",
        "                \"patch\",\"championId\",\"championName\",\"games\",\"wins\",\n",
        "                \"win_rate\",\"pick_rate\",\"ban_rate\",\"cluster\",\"tier\"\n",
        "            ])\n",
        "            w.writeheader()\n",
        "            w.writerows(all_rows)\n",
        "        print(f\"[saved] {combined_csv}\")\n",
        "\n",
        "    if all_diags:\n",
        "        diag_out = out_dir / \"kmeans_validation_all_patches.csv\"\n",
        "        pd.DataFrame(all_diags).to_csv(diag_out, index=False)\n",
        "        print(f\"[saved] {diag_out}\")\n",
        "\n",
        "    if not all_rows:\n",
        "        print(\"[warn] no tierlists produced ‚Äî check K and data size.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ‚ñ∂Ô∏è All-Patches Launcher\n",
        "import sys, runpy\n",
        "CSV=CSV_COMBINED; K=5; LOGIT=True\n",
        "argv=[\"tierlist_all_patches.py\",\"--csv\",CSV,\"--k\",str(K),\"--out-dir\",OUT_DIR]\n",
        "if LOGIT: argv.append(\"--logit\")\n",
        "sys.argv=argv\n",
        "runpy.run_path(\"/mnt/data/tierlist_all_patches.py\", run_name=\"__main__\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
